\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{parskip}
\usepackage{graphicx}

%\usepackage{easyReview} %used for annotation. Cannot be run together with (changes) - command "highlight" defined twice
\usepackage{changes}%use while actively annotating

%\usepackage[final]{changes} % once we are happy with all annotation, we can accept all proposed changes and show only the final document.

% simply select text you want to annotate and start typing the command \replacedGM{the selected text will appear here}{and here you enter the new replacement text}
\definechangesauthor[name={Timo}, color=orange]{tp}
\definechangesauthor[name={Giovanni}, color=green]{gm}
\newcommand{\replacedTP}[2]{\replaced[id=tp]{#2}{#1}}
\newcommand{\addedTP}[1]{\added[id=tp]{#1}}
\newcommand{\deletedTP}[1]{\deleted[id=tp]{#1}}
\newcommand{\highlightTP}[1]{\highlight[id=tp]{#1}}

\newcommand{\replacedGM}[2]{\replaced[id=gm]{#2}{#1}}
\newcommand{\addedGM}[1]{\added[id=gm]{#1}}
\newcommand{\deletedGM}[1]{\deleted[id=gm]{#1}}
\newcommand{\highlightGM}[1]{\highlight[id=gm]{#1}}

%opening
\title{Foundation of Econometrics - Homework}
\author{Giovanni Maffei, Timo Pred√∂hl}

\begin{document}
	
	\maketitle
	
	\begin{spacing}{1}
		\tableofcontents		
	\end{spacing}
	
	\pagebreak

	\section{Exercise 1: Spurious Regressions}
	\subsection{Set-up and Definitions}
	\begin{itemize}
		\item We are studying two independent stochastic processes: $y_t$ and $x_t$  with $t=1,...,T$.
		\item Each characterization of these processes is assumed to have independent errors: $u_{y,t} \overset{iid}{\sim}N(0,1)$ and $u_{x,t} \overset{iid}{\sim}N(0,1)$.
			\subitem Both errors are standardized random walk stochastic processes.
		\item The different regression modeles we are testing here result from the two processes below:
			\subitem $y_t = y_{t-1} + u_{y,t}$ and $x_t = x_{t-1} + u_{x,t}$ \vspace{0.5em}\\ 
			$x_t$ and $y_t$ are integrated of order 1: I(1) and contain a unit root.
			\subitem $y_t = 0.8y_{t-1} + u_{y,t}$ and $x_t = 0.8x_{t-1} + u_{x,t}$.\vspace{0.5em}\\
			Both processes are represented as stationary AR(1) models\footnote{$\phi = 0.8 < \,\mid1\mid$} with an ergodic expected value of 0, as no intercept is modelled into those.
		\item Each regression model is estimated with OLS under asymptotic theory\footnote{Indeed, we are not assuming strict exogeneity of the regressors as we are using lagged values of the dependent variable as a regressor.}:
			\subitem A.1: the combination of $(y_t, k_t)$ for $t = 1,...,T$ is IID, hence $(v_t, k_t)\sim IID$\footnote{By $K$ we refer to the matrix of the regressors and $v_t$ corresponds to the error term in each model.}.
			\subitem A.2: regressors uncorrelated with the simultaneous error term, hence $E[v_{t}k_{t}]=0_J$\footnote{J represents the total number of regressors.}.
			This assumption is not restricting the utilization of lagged values of the dependent variable into the regressors matrix.
			\subitem A.3: The VaR-Cov matrix of the regressors: $Q= E[k_{t}k_{t}']$ exists, is positive definite and has rank corresponding to the total number of regressors (no regressor is a linear combination of the others).
			\subitem A.4: $E[v_{t}^2k_{t}k_{t}']$ exists and is positive definite.
		\item The Monte Carlo exercise is based on $N_{sim} = 10,000$.
		\item We will test each regression model assuming different samples of pairs of observations: $t=6, 12, 60, 120, 240, 360, 480$.      	 	
	\end{itemize}
	\subsection{Part 1: Replication of the exercise for Figure 14.1 in Davidson MacKinnon (2005, book)}
	The four data generating processes (DPG) we are studying are:
	\begin{itemize}
	\item $DPG_1$: $y_{t} = \beta_{1} +\beta_{2}x_{t} + v_{t}$
		\subitem We assume that both $x_t$ and $y_t$ are I(1).
	\item $DPG_2$: $y_{t} = \beta_{1} +\beta_{2}x_{t} + v_{t}$
		\subitem We assume that both $x_t$ and $y_t$ are stationary AR(1) with $\phi = 0.8$.
	\item $DPG_3$: $y_{t} = \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}$
		\subitem We assume that both $x_t$ and $y_t$ are I(1).
	\item $DPG_4$: $y_{t} = \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}$
		\subitem We assume that both $x_t$ and $y_t$ are stationary AR(1) with $\phi = 0.8$. 	
	\end{itemize}	
	In light of the hypothesis formulated in the previous section, we would expect that the estimates of $\beta_{2}$ and $R^2$ were not significantly different from 0\footnote{Such consideration for the $R_2$ does not apply directly to DPGs 3 and 4 where we are regressing on $L(y_{t})$.}.
	Out of the Monte Carlo simulation performed by Davidson and MacKinnon (2005, book), we present three results\footnote{The R code is available as an attachment to this submission.}:
	\begin{itemize}
		\item Distribution of the $R_2$ for each sample size $t$ \\
		Placeholder for chart 1! 
		\item Distribution of the realizations of the t-statistics for the test of hypothesis: $H_0: \beta_2 =0$ \\
		Placeholder for chart 2!
		\item Empirical rejection frequencies calculated as the ratio between the number of times $H_0$ got rejected against the variable $t$, assuming 5\% as significance level. The size of the Monte Carlo test ($N_{sim}$) corresponds to the total number of observations used to calculate the empirical rejection ratio for each point in the figure below. Hence, the higher this parameter and the lower the dispersion of the results in repeated samples of this simulation will be. \\  
		Placeholder for chart 3!
	\end{itemize}

	\subsection{Part 2: Summarize the problems of spurious regressions in econometrics}
	The results obtained are different from what we previously expected. Indeed, by looking at Figure 1.3, we can observe that:
	\begin{itemize}
		\item Apart from DPG 4 whose $H_0$ rejection rate converges asymptotically to 5\%, the empirical rejection rate is structurally higher than the significance level for the three other processes.
		\item Additionally, we notice that the proportion of rejections keeps increasing alongside the sample size for DPG 1 always implying a statistically significant relationship bwetween $y_t$ and $x_t$ even if this does not exist.
		\item Therefore, it emerges that the actual probability of a Type 1 error\footnote{$P(R_{H_0}|H_0)$.} is significantly higher than the assumed test size.
	\end{itemize}
	Usually, the bias of the $\beta$ multipled by $n^{0.5}$ is $O_{p}(1)$, as per below demonstration and noting that $K'K = O_{p}(T)$ and $K'v = O_{p}(T^{0.5})$:
	\begin{align*}
	\hat{\beta} - \beta &= (K'K)^{-1}K'v \\
	T^{0.5}(\hat{\beta} - \beta) &= T^{0.5}((K'K)^{-1}K'v) \\
	&= T^{0.5}O_{p}(T^{-1}O_{p}(T^{0.5}) \\
	&= O_{p}(1)  
	\end{align*}
	The bias is eventually bounded for $T \to +\infty$, so it does not explode asymptotically.\\ 
	However, we have here an issue of unit roots and spurious regressions. If, at least, one of the regressors is a unit root\footnote{This happens for DPG 1 and 3, although for the latter the proportion of rejections does not coverge to 1 as $t$ increases. This is because we are reducing $y_t$ by one lag turning the dependent variable into a white noise random walk.}, we can derive that $E[K_{t}'K_{t}]$ is not a finite positive definite matrix anymore and there is a violation of the OLS assumptions under asymptotic theory (steps below are for DPG 1\footnote{$V(x_t) = E(x_{t}^2) = t$.}, same holds in case of multiple regressors):
	\begin{align*}
	E[x_{T}'x_{T}] &= \sum_{t=1}^{T}x_{t}^2 \\  
	&= T + (T-1) +...+2+1 \\
	&= \frac{T(T+1)}{2}
	\end{align*}
	It follows that: i) $T^{-1}x_{T}'x_{T}$ is $O(T)$ and ii) this metric does not have a finite probability limit ($T \to + \infty$). \\
	The distribution of the t-statistics does not converge to the Student's $t$ even asymptotically causing an overrejection of the null.\\
	Eventually, we observe that the issue of spurious regressions occurs even if all variables are stationary\footnote{AR(1) in this case.}.
	Although the proportion of rejections converges to a fixed number and $E[K_{t}'K_{t}]$ is a finite definite positive metric, we can see that the result for DPG 2 implies an overrejection of the null while the empirical frequency for DPG 4, the only correct regression model, converges to the $5\%$ significance level only after increasing significantly the sample size (in the order of a few thousand observations). \\  
	The distortion for DPG 3 arises from the fact that neither the constant nor $x_t$ have any explanatory power for $y_t$, therefore $y_t = v_t = 0.8y_{t-1}+u_{y,t}$.\\ As $y_t$ is modelled as an AR(1), the error term of DPG 3 becomes:
	\begin{align*}
	v_t &= 0.8y_{t-1}+u_{y,t} \\
	v_t &= 0.8(u_{y,t-1} + 0.8y_{t-2}) + +u_{y,t} \; t=1,...,T
	\end{align*}
	This intuition suggests that there is serial autocorrelation in the errors and the (asymptotic) solution to avoid too small standard errors in finite samples is to switch from the OLS VaR-Cov estimator to the Newey-West heteroskedasticity and autocorrelation consistent (HAC) one.
	\section{Exercise 2: Univariate Time Series Regression and Jensen's alpha}
	\subsection{Set-up and Definitions}
	\begin{itemize}
		\item We are assuming a timeseries regression for one asset: $i=1,...,N$, with N corresponding to the total number of securities available in the investable universe:
			\begin{align} \label{timeseries}
				r_{it} &= \alpha_i + \beta_{im} z_{mt} + u_{it} \quad t = 1,...,T \\ 
				u_i|z_{mt} &\overset{iid}{\sim} (0, w_{i}^2) \notag \\ \notag
			\end{align} \vspace{-3em}
		\item $X$ is the matrix of the regressors.	
		\item $E[x_ix_i']$ exists, and is definite positive.	
		\item ${Rank}(X)$ = 2 (no redunant regressors), hence $(X'X)^{-1}$ exists.	
		\item $\hat{\alpha}_i$ and $\hat{\beta}_{im}$ are the OLS estimators of $\alpha$ and $\beta$ in \ref{timeseries}.
		\item $z_{mt}$ corresponds to the return of a market index (used as proxy of the market portfolio) at time $t$. 
	\end{itemize}
	\subsection{Part 1: Demonstrate that $\hat{\alpha}_i = \overline{r}_{i} - \hat{\beta_i}\overline{z}_{m}$ and $\hat{\beta_i} = \frac{\hat{\sigma}_{im}}{\hat{\sigma}_{m}^2}$} 
	Noting that if we substitute the intercept by adding $1_T$ as the first column of the regressors matrix: $X$, we can rewrite \ref{timeseries} in the following matricial representation: $r_{i,(1\times T)} =X_{i,(T\times 2)} \beta_{i,(2\times 1)} + u_{i,(T\times1)}$, with X  defined as a (1 x 2) matrix: $[1_{(T x 1)},z_{m,(T x 1)}]\footnote{The second term of each matrix subscript is meant to represent the dimension of the corresponding metric for the avoidance of doubt, we will drop such notation for tractability reasons.}$.\\
	Additionally, recalling that: i) the OLS estimator of $\alpha_i$ and $\beta_{im}$ corresponds to: $(X'X)^{-1}X'r_i$, ii) $1_{T}'1_T=T$ and iii) the inverse of $(X'X)^{-1}$ is:
	\begin{align} \label{(X'X)^-1}
		\begin{bmatrix}
		a & b \\ c & d
		\end{bmatrix}
		^{-1} &= \frac{1}{ad-bc}
		\begin{bmatrix}
		d & -b \\ -c & a
		\end{bmatrix} \notag \\
			(X'X)^{-1} &= \frac{1}{T\sum_{t=1}^{T}z_{mt}^2-(\sum_{t=1}^{T}z_{mt})^2} \begin{bmatrix} \sum_{t=1}^{T}z_{mt}^2 & -\sum_{t=1}^{T}z_{mt} \\ -\sum_{t=1}^{T}z_{mt} & T \end{bmatrix} 
	\end{align}
	And observing $X'r_{i} = \begin{bmatrix} \sum_{t=1}^{T}r_{it} \\ \sum_{t=1}^{T}z_{mt}r_{it} \end{bmatrix} $, we derive that:
	\begin{align} \label{derivation}
	\hat{\beta_i} &= (X'X)^{-1}X'r_i \notag \\
				&= \begin{bmatrix}
				\frac{T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}^2) T(\frac{1}{T}\sum_{t=1}^{T}r_{it}) - T(\frac{1}{T}\sum_{t=1}^{T}z_{mt})T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}r_{it})}{T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt}^2)-T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt})^2} \\ \frac{T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt}r_{it}) - T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}) T(\frac{1}{T}\sum_{t=1}^{T}r_{it})}{T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt}^2)-T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt})^2} 
				\end{bmatrix}
	\end{align}
Besides, we can further refine this expression by noting:
\begin{itemize}
\item $\frac{1}{T}\sum_{t=1}^{T}z_{mt} =\overline{z}_m $
\item $\frac{1}{T}\sum_{t=1}^{T}r_{it} =\overline{r}_i $
\item $T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt}^2)-T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt})^2 = T^2\hat{\sigma}_{m}^2 $, with $\hat{\sigma}_{m}^2$ as the variance of the market index returns.
\item $T^2(\frac{1}{T}\sum_{t=1}^{T}z_{mt}r_{it}) - T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}) T(\frac{1}{T}\sum_{t=1}^{T}r_{it}) = T^2\hat{\sigma}_{z,i}$, with $\hat{\sigma}_{z,i} $ as the covariance between the returns of the single asset and the market index.
\item $T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}^2) T(\frac{1}{T}\sum_{t=1}^{T}r_{it}) - T(\frac{1}{T}\sum_{t=1}^{T}z_{mt})T(\frac{1}{T}\sum_{t=1}^{T}z_{mt}r_{it}) = \\ 
= T^2\overline{r}_i(\frac{1}{T}(\sum_{t=1}^{T}z_{mt}^2-\overline{z}_m{^2})- T^2\overline{z}_m(\frac{1}{T}(\sum_{t=1}^{T}z_{mt}r_{it}-\overline{z}_m\overline{r}_i) \\
= T^2(\overline{r}_i\hat{\sigma}_{m}^2 -\overline{z}_m\hat{\sigma}_{z,i})$
\end{itemize}
By substituting the formulae above into \ref{derivation}, we derive:
\begin{align}
			\hat{\beta_i} &= \begin{bmatrix} \hat{\alpha}_i \\ \hat{\beta}_{im} \end{bmatrix} \notag \\
 			&= \begin{bmatrix} \frac{\overline{r}_i\hat{\sigma}_{m}^2-\overline{z}_m\hat{\sigma}_{z,i}}{\hat{\sigma}_{m}^2} \\ \frac{\hat{\sigma}_{z,i}}{\hat{\sigma}_{m}^2} \end{bmatrix} \notag \\
 			&= \begin{bmatrix} \overline{r}_i - \overline{z}_m\hat{\beta}_{im} \\ \frac{\hat{\sigma}_{z,i}}{\hat{\sigma}_{m}^2} \end{bmatrix}
\end{align}
\subsection{Part 2: Demonstrate the asymptotic distribution of $\hat{\beta_i}|z_m$}
As we are not assuming any specific form for $u_i|z_{mt}$, we will study the distribution of $\hat{\beta_i}|z_m$ asymptotically ($T \to \infty$). Therefore, by rearranging the definition of $\hat{\beta_i}$ we derive:  
\begin{align*}
\hat{\beta_i} | z_{mt} &= (X'X)^{-1}X'r_i = \beta_i + (X'X)^{-1}X'u_i \\
\sqrt{T}(\hat{\beta_i} -\beta_i) &= \sqrt{T}(X'X)^{-1}X'u_i \\
&= (\frac{X'X}{T})^{-1}\sqrt{T}X'u_i 
\end{align*}
Further noting that (under $T \to +\infty$):
\begin{itemize}
\item $(\frac{X'X}{T}) \overset{p}{\to} E[x_ix_i']$ due to the application of the Law of Large Numbers (LLN) because $[x_{it}]_{t=1}^T$ is a sequence of IID observations with finite expected value and variance. Additionally, by using the Slutsky's theorem we obtain: $(\frac{X'X}{T})^{-1} \overset{p}{\to} E[x_ix_i']^{-1}$. 	
\item $X'u_i \overset{d}{\to} X'N(0,w_i^2I_T)X$ due to the application of the Central Limit Theorem (CLT) in light of: i) IID observations, ii) errors uncorrelated with the regressors and iii) finite moments\footnote{As mentioned in the \textit{Set-up and Definitions} part of this exercise.}.
\end{itemize}
By applying the properties of the multivariate Normal distribution, we obtain:
\begin{align*}
\hat{\beta_i} \overset{d}{\to} N(\beta_i, (X'X)^{-1}X'w_i^2I_TX(X'X)^{-1})
\end{align*}
Simplifying the variance term of the distribution of $\hat{\beta_i}$:
\begin{align*}
V(\hat{\beta_i}) &= (X'X)^{-1}X'w_i^2I_TX(X'X)^{-1} \\
&= w_i^2 (X'X)^{-1}
\end{align*}
Additionally, we can rewrite \ref{(X'X)^-1} as per below:
\begin{align} \label{X'X rearranged}
(X'X)^{-1} &=  \frac{1}{T^2\hat{\sigma}_{m}^2} 
				\begin{bmatrix} (\sum_{t=1}^{T}z_{mt}^2) -T\overline{z}_m^2 + T\overline{z}_m^2 & -T\overline{z}_m \\ -T\overline{z}_m & T \end{bmatrix} \notag \\
			&= 	\frac{1}{T^2\hat{\sigma}_{m}^2}
			    \begin{bmatrix} (T\hat{\sigma}_{m}^2 + T\overline{z}_m^2) & -T\overline{z}_m \\ -T\overline{z}_m & T \end{bmatrix} \notag \\
			&=  \frac{1}{T} 
				\begin{bmatrix} (1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2}) & \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} \\ \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} & \frac{1}{\hat{\sigma}_{m}^2} \end{bmatrix}   
\end{align}
Eventually, we conclude this demonstration by expanding $\hat{\beta}_i$ into its components and by replacing \ref{X'X rearranged} into $\hat{\beta}_i \overset{d}{\to} N(\beta_i,V(\hat{\beta_i})$:
\begin{align}
\begin{bmatrix} \hat{\alpha}_i \\ \hat{\beta}_{im} \end{bmatrix}
\sim N(\begin{bmatrix} \alpha_i \\ \beta_{im} \end{bmatrix}, \frac{w_i^2}{T}\begin{bmatrix} (1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2}) & \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} \\ \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} & \frac{1}{\hat{\sigma}_{m}^2} \end{bmatrix}) 
\end{align}
\subsection{Part 3: Demonstrate the t-statistics to test $H_0$: $\alpha_i = 0 $}
We add below a few considerations additional to what presented in \textit{Set-up and Definitions} to support this proof:
\begin{itemize}
\item We are studying the asymptotic distribution of this test under $T \to +\infty$\footnote{Indeed, no distribution is provided for the errors in \ref{timeseries}.}.	
\item This is a linear hypothesis test, with $m = 1$ linear restrictions tested\footnote{The only restriction we are imposing on $\beta_i$ is: $\alpha_i = 0 $.}.
\item The generic matrix R (m x K), formalizing the linear restrictions on $\beta$, corresponds to the vector $\begin{bmatrix} 1 & 0 \end{bmatrix}$.
\item The generic vector q (m x 1) represents the restricted values imposed on $\beta$. As we are imposing only one restriction, q corresponds to a scalar metric: 0.
\item As we don't know the actual variance of $u_i|z_{mt}$ but we estimate the corresponding OLS residuals, we utilize $\frac{1}{T}\sum_{t=1}^{T}\hat{u}_{it}^2$ as an asymptotically unbiased estimator by applying the LLN\footnote{$\overline{u}_i = 0$ because of the inclusion of the intercept in $X$.}.
\end{itemize}
Under the null hypothesis $H_0$: $R\beta_i=q$:
\begin{align*}
R\hat{\beta_i}-q = R(\hat{\beta_i} - \beta_i)
\end{align*}
Furthermore, as proved in the previous exercise and by applying the properties of the multivariate Gaussian distribution, we derive:
\begin{align} \label{2.3}
R(\hat{\beta}_i-\beta_i) &\sim N(0, RV(\beta_i)R') \notag \\
[RV(\beta_i)R']^{-\frac{1}{2}}R(\hat{\beta}_i-\beta_i) &\overset{d}{\to} N(0,I_m)
\end{align}
By substituting for $R=\begin{bmatrix} 1 & 0 \end{bmatrix}$ in $RV(\beta_i)R'$ and rearranging the terms:
\begin{align*}
RV(\beta_i)R' &= \frac{w_i{^2}}{T} \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} (1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2}) & \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} \\ \frac{-\overline{z}_m}{\hat{\sigma}_{m}^2} & \frac{1}{\hat{\sigma}_{m}^2} \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
&= \frac{w_i{^2}}{T}(1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2})
\end{align*}
Therefore \ref{2.3} becomes:
\begin{align*}
[RV(\beta_i)R']^{-\frac{1}{2}}R(\hat{\beta}_i-\beta_i) &= \frac{R(\hat{\beta}_i-\beta_i)}{w_i\sqrt{\frac{(1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2})}{T}}} \\
&= \frac{\hat{\alpha}_i - \alpha|H_0}{w_i\sqrt{\frac{(1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2})}{T}}} \\
&= \frac{\hat{\alpha}_i}{w_i\sqrt{\frac{(1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2})}{T}}}
\end{align*}
Eventually by using the unbiased estimator for $w_i^2$, as mentioned at the beginning of this proof, we derive\footnote{$I_m$ is the scalar 1, hence $N(0,I_m) = N(0,1)$.}: 
\begin{align}
\frac{\hat{\alpha}_i}{\hat{w}_i\sqrt{\frac{(1 + \frac{\overline{z}_m^2}{\hat{\sigma}_{m}^2})}{T}}} \overset{d}{\to} N(0,1)
\end{align}

\section{Exercise 3: Pricing errors in FM}
	\subsection{Set-up and Definitions}
	\begin{itemize}
		\item The data generating process (DGP) for the returns in excess of the risk-free rate with $\MakeUppercase\kappa$ tradable factors is: 
			\begin{align} \label{cross-sectional}
				r_{t} &= \MakeUppercase\beta f_t + u_t \quad t = 1,...,T 
			\end{align} \vspace{-1.75em} 
			\item $r_t$ is the (N x 1) vector containing the excess return of each security at time $t$: $r_{i,t}$ for i = 1,...,N, with N corresponding to the total number of securities available in the investable universe.   
			\item $u_t$ is the (N x 1) vector containing the errors of the DGP and is assumed: $\overset{iid}{\sim} \: (0,\Omega)$ over $t$ with finite matrix $\Omega$. 
			\item $\MakeUppercase\beta$ is the (N x K)\footnote{N is deemed to be larger than K.} matrix reporting on each line the factor loadings for i = 1,...,N\footnote{We are including the vector of ones: $i_N = [1,1,...,1]'$ as the first column of $\MakeUppercase\beta$ in order to add the intercept to the DGP introduced above.}.
			\subitem Rank($\MakeUppercase\beta$) = K therefore, the inverse of $\MakeUppercase\beta'\MakeUppercase\beta$ exists and is definite positive. 
			\item $f_t$ is the (K x 1) vector of the excess returns of the tradable risk factors in t, distributed as per $\overset{iid}{\sim} \: (\lambda,\Omega_{f})$ over $t$. $\lambda$ represents the (K x 1) vector of the true risk premia: $E(f_t) = \lambda$.\footnote{As we are including the intercept in the DGP and $r_{t}$ is the vector of the excess returns, we would expect $\lambda_{1,t}$ to be equal to 0.}
			\item $u_s$ is independent from $f_t \; \forall(s,t)$ hence, $E[u_{s}f_t] = 0$ (hypothesis of strict exogeneity of the regressors).
			\item We assume that the true factor loadings are known, hence we dont face a problem of error in variables (EIV) for this DPG\footnote{If $\MakeUppercase\beta$ had not been known, we would have estimated the vector from the N timeseries regressions covering the entire sample of lenght: $T$.}.
			\item $\hat{\lambda}$ is estimated via OLS and corresponds to: $(\beta'\beta)^{-1}\beta'r_t$.
			\end{itemize}
	\subsection{Part 1}
	Demonstrate that $\hat{\epsilon} \overset{p}{\to} 0$ as $T\to{+\infty}$ 
	Given the OLS formula for the pricing errors of \ref{cross-sectional} is $\hat{\epsilon}_t = r_t - \beta\hat{\lambda}_t$ and replacing $\hat{\lambda}$ with the corresponding OLS result, we find that $\hat{\epsilon}_t = r_t - \beta(\beta'\beta)^{-1}\beta'r_t = M_{\beta}r_t$\footnote{$M_{\beta}$ corresponds to $I_N - \beta(\beta'\beta)^{-1}\beta$ which is symmetric and idempotent ($I_N$ is the identity matrix (N x N)).}.\\
	Therefore we observe that:
	\begin{align} \label{3.1.a}
				\hat{\epsilon} &= \frac{1}{T}\sum_{t=1}^{T}\epsilon_t = \frac{1}{T}\sum_{t=1}^{T}M_br_t
	\end{align}
	Looking at \ref{3.1.a} and replacing $r_t$ with \ref{cross-sectional}, we derive:
\begin{align} \label{3.1.b}
				\hat{\epsilon} &=\frac{1}{T}\sum_{t=1}^{T}M_br_t \notag \\ \notag
				&= \frac{1}{T}\sum_{t=1}^{T}M_b(\MakeUppercase\beta f_t + u_t) \\ 
				&= \frac{1}{T}\sum_{t=1}^{T}M_b(\MakeUppercase\beta f_t) + \frac{1}{T}\sum_{t=1}^{T}M_bu_t
\end{align}
The first component of \ref{3.1.b} is equal to 0 as it becomes: $\frac{1}{T}\sum_{t=1}^{T}(\MakeUppercase\beta f_t-\MakeUppercase\beta f_t)$, while for the second term we see:
\begin{align*} 
				\hat{\epsilon} &=\frac{1}{T}\sum_{t=1}^{T}M_bu_t \\
				&= M_b(\frac{1}{T}\sum_{t=1}^{T}u_t)
\end{align*}
As $u_t \overset{iid}{\sim} \: (0,\Omega)$ and $\Omega$ is finite, we can apply the Law of Large Numbers (LLN) and obtain that $(\frac{1}{T}\sum_{t=1}^{T}u_t) \overset{p}{\to} 0$, it follows that: $\hat{\epsilon} \overset{p}{\to} 0$.
\subsection{Part 2} 
Demonstrate that $\frac{1}{T}\sum_{t=1}^{T}(\hat{\epsilon}_t-\hat{\epsilon})(\hat{\epsilon}_t-\hat{\epsilon})' \overset{p}{\to} M_\beta\Omega M_\beta$ as $T\to{+\infty}$
Following the same setup and results of 1.2 and recalling that $(M_\beta)' = M_\beta$, we derive that:
\begin{align} \label{3.1.c}
\frac{1}{T}\sum_{t=1}^{T}(\hat{\epsilon}_t-\hat{\epsilon})(\hat{\epsilon}_t-\hat{\epsilon})' &= \frac{1}{T}\sum_{t=1}^{T}(M_\beta r_t)(M_\beta r_t)' \notag\\
&= \frac{1}{T}\sum_{t=1}^{T} M_\beta(\beta f_t + u_t)(\beta f_t + u_t)'M_\beta \notag\\
&= \frac{1}{T}\sum_{t=1}^{T}M_\beta (\beta f_t f_t' \beta' + u_t u_t' + 2\beta f_t u_t') M_\beta  
\end{align} 
Additionally, we can decompose \ref{3.1.c} into the following components:
 \begin{itemize}
 \item $\frac{1}{T}\sum_{t=1}^{T}M_\beta(\beta f_t f_t' \beta') M_\beta = \frac{1}{T}\sum_{t=1}^{T}(\beta f_t f_t' \beta' - \beta f_t f_t' \beta')M_\beta = 0$ 
 \item $\frac{1}{T}\sum_{t=1}^{T}M_\beta(2\beta f_t u_t')M_\beta = \frac{2}{T}\sum_{t=1}^{T}(\beta f_t u_t'-\beta f_t u_t')M_\beta = 0$
 \item $\frac{1}{T}\sum_{t=1}^{T}M_\beta u_t u_t' M_\beta$
 \end{itemize}
 Assuming: i) the fourth moment of $u_t$ exists and is finite and ii) recalling that $u_t \overset{iid}{\sim} \: (0,\Omega)$, we can apply the LLN on $\frac{1}{T}\sum_{t=1}^{T} u_t u_t'$ and observe that this quantity $\overset{p}{\to} \Omega$.
 It follows that the last member of \ref{3.1.c} $\overset{p}{\to} M_\beta \Omega M_\beta$, therefore:
\begin{align} \label{3.2.b}
\frac{1}{T}\sum_{t=1}^{T}(\hat{\epsilon_t}-\hat{\epsilon})(\hat{\epsilon_t}-\hat{\epsilon})' \overset{p}{\to} M_\beta \Omega M_\beta \\ \notag 
\end{align}    
\end{document}
