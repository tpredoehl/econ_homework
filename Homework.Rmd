---
title: "EDHEC PhD Finance 2022 - Econometrics Homework"
author: "Giovanni Maffei, Timo Predoehl"
output:
  pdf_document: default
  html_document:
    df_print: paged
  html_notebook: default
---

```{r set_up, message=TRUE, warning=TRUE, include=FALSE, paged.print=FALSE}
pkgs.installed <- installed.packages()
pkgs.required <- c("car", "AER", "stargazer", "tictoc", "quantmod", "xts", "readr", "latex2exp", "gridExtra", "summarytools", "qwraps2", "nortest", "moments", "xtable", "sm", "astsa", "tseries", "ggplot2", "ggplotgui", "shiny", "tidyverse")
pkgs.missing <- pkgs.required[which(!pkgs.required %in% pkgs.installed)]
lapply(pkgs.missing, install.packages, character.only = TRUE)
lapply(pkgs.required, require, character.only = TRUE)

rm(list=ls()) # clear all variables from enviroment/workspace
```


\section*{Question 1: Spurious Regressions (2 points)} 
\begin{itemize}
	\item \textbf{compulsory} ( a ) [1 Point]
	\begin{itemize}
		\item{i} Replicate the analysis leading to Figure 14.1 in Davidson MacKinnon (2005, book) by running 100,000 MC, or simply 10,000 MC if your computer is slow (instead of 1 million), and using T = 6, 12, 60, 120, 240, 360, 480. (i) Compute also for each sample size T the distribution of the $R^2s$ of the MC simulations with either 7 separate histograms, or one unique figure where you report on the y-axis the 5\%, 10\% 25\%, 50\%, 75\%, 90\% and 95\% quantiles of the distributions of the simulated $R^2$, and on the x-axis you have T = 6, 12, 60, 120, 240, 360, 480.
		
		\item{ii} Similarly (either with histograms, or with one plot of the quantiles) report the distributions of the estimates t-statistics for the test of the null H0 : $\beta_2$ = 0 and
		
		\item{iii} their empirical rejection frequencies (that is the empirical size of the tests),which is exactly the figure 14.1 in Davidson MacKinnon (2005, book).
	\end{itemize}
	\item ( b ) [1 Point] Based on the results obtained by answering to point (a) summarize the problems of spurious regressions in econometrics.
\end{itemize} 

\subsection*{(a)(i) Replication Davidson McKinnon, 2005, fig 14.1}

Simulate a random walk processes $y_t = y_{tâˆ’1} + e_t, y_0 = 0, e_t \sim IID(0, \sigma^2)$ analog to (14.03)
```{r rw}
# Simulate : White noise, Random Walk
set.seed(102)
sigma_ux <- 1 ; 
T <- 300      # T = sample size (length of TS to simulate)
# Simulate white Noise: u_{xt} ~ iidN(0,1), for t = 1, ..., T   
u = rnorm(T+1, mean = 0, sd = sigma_ux);  
# Loop to simulate observations of Random Walk : X_t = X_{t-1} + u_{xt} , 
X.RW      <- NA*matrix(0,T+1,1) # empty vector ( T X 1 )
X.RW[1]   <- 0 # initial value of RW set arbitrarily = 0 (unconditional mean)

# 'for' loop to generate all observations of RW form 2 to T
for (t in 2:(T+1)){ X.RW[t] <- X.RW[t-1] + u[t] }
X.RW <- X.RW[-1,] # delete initial observation which was set to 0    

# plot simulated RW (X_t) and its innovations (u_xt)
# dev.off()
plot(X.RW, type='o', pch=19, col="blue", main=TeX('White Noise and Random Walk'), xlab='time', ylab='', cex = 0.5)
abline(0,0)
lines(u, type='o', pch=19, col="red", cex = 0.5)
legend("bottomleft", legend=c(TeX('RW: $X_t = X_{t-1} + u_{x,t}$   '), TeX('WN: $u_{x,t} \\sim iidN(0,1)$   ')), 
       pch=c(19,19), col=c("blue", "red"));

```

Simulate two AR(1) processes according to $x_t = \phi x_{t-1} + u_t$, with $\phi=0.5$ and with $\phi = 1$. The ACFs of two processes indicate that in latter significant autocorrelation persists at least up to $x_{t-20}$, while the former autocorrelation becomes insignificant after 3 lags.

```{r ar}
# Simulate AR(1) and plot acf
set.seed(102)
sigma_ux <- 1 ; 
T<- 200
u    <- rnorm(T+1, mean = 0, sd = sigma_ux) #WN
phi  <- 0.5                                 # AR 1 coefficient 

X.AR1.stat    <- NA*matrix(0,T,1) # empty vector ( T X 1 )
X.AR1.nonstat <- NA*matrix(0,T,1) # empty vector ( T X 1 )

X.AR1.nonstat[1] <- 0 # both processes start at t= 0 from the value 0
X.AR1.stat[1]    <- 0  
for (t in 2:T){
  X.AR1.nonstat[t] <- X.AR1.nonstat[t-1] + u[t];
  X.AR1.stat[t]    <- phi*X.AR1.stat[t-1] + u[t] 
}

y.range <-range(as.matrix(c(X.AR1.stat,X.AR1.nonstat)),na.rm = TRUE, finite = FALSE) 

#dev.off()
par(mfrow=c(2,2)) # 4 whiten noises
plot(X.AR1.nonstat, type='o', pch=19, col="blue", main=TeX('AR(1): $X_t = X_{t-1} + u_{x,t},  u_{x,t} \\sim iidN(0,1)$'), xlab='time', ylab='', cex = 0.5)
abline(0,0)
plot(X.AR1.stat, type='o', pch=19, col="red", main=TeX('AR(1): $X_t = 0.5 X_{t-1} + u_{x,t},  u_{x,t} \\sim iidN(0,1)$'), xlab='time', ylab='', cex = 0.5)

# plot Autocorrelation function (acf)
lag.max.acf = 20;
acf(X.AR1.nonstat, main=TeX('ACF, AR(1) NON-stationary'), lag.max = lag.max.acf, xlab = "lag ") #, xlim = c(1,40)
acf(X.AR1.stat, main=TeX('ACF, AR(1) stationary'), lag.max = lag.max.acf, xlab = "lag ") # xlim = c(1,40)
```

```{r}
###############################################
# Simulate two independent RW and run spurious regression

# function to simulate K independent AR process with :
# T       = n.f simulated dates (simulated sample size)
# phi_vec = [\phi_1, ..., phi_N]' = autoregressive coefficients for each AR process
# c_vec   = [c_1, ..., c_N] = constant in AR process
independent_AR_simulation<- function(phi_vec, c_vec, sigma_u_vec, T){

  N <- length(phi_vec) ; # number of AR(1) processes to simulate
  
  # create empty matrix T X N that will contain all simulated AR processes
  
  AR_sim_mat <- NA*matrix(0,T+1,N) # empty vector ( (T+1) X 1 )
   u_sim_mat <- NA*matrix(0,T+1,N) # empty vector ( (T+1) X 1 )
  
  # loop for different AR processes
  for (ind1 in seq(1,N)) {
    #########################
    # loop for different dates for one AR process

    c       <-       c_vec[ind1]
    phi     <-     phi_vec[ind1]
    sigma_u <- sigma_u_vec[ind1]

    # simulate vector of innovations zero mean, and sd given as input
#    set.seed(100+ind1) # fix "seed" in order to produce same random numbers every time next line of code is called!
    u    <- rnorm(T+1, mean = 0, sd = sigma_u) #WN
    u_sim_mat[,ind1] <- u

    # generate starting observation for the process: unconditional mean (stationary processes) or 0 (for non-stat processes only!)
    if (abs(phi)<1){
    AR_sim_mat[1,ind1] <- c/(1-phi) ; #start from unconditional value of the process
    } else{
    AR_sim_mat[1,ind1] <- 0 ; #start from unconditional value of the process
    }

    # simulate all other observations t=2 to T+1
    #######################
    for (t in seq(2,T+1)) {
      AR_sim_mat[t,ind1] <- phi*AR_sim_mat[t-1,ind1] + u[t];
      # print(c(t,ind1))
      
      }# end of t loop
    ########################

    } # end of ind1 loop
    #########################

  # Output of function
  # 1 = simulated AR process
  # 2 = simulated innovations
  
  return(list(AR_sim_mat = AR_sim_mat, u_sim_mat = u_sim_mat))
  } # end of function
########################
  
# RUN FUNCTION #############################################
phi_vec     = c(0.95, 0.95) 
c_vec       = c(0,0)
sigma_u_vec = c(1,1) 
T           = 60


output_temp <- independent_AR_simulation(phi_vec, c_vec, sigma_u_vec, T)
str(output_temp)

AR_sim <- output_temp$AR_sim_mat
u_sim  <- output_temp$u_sim_mat
########################################################
# plot simulated innovations of AR(1), that is u_{x,t} and $u_{y,t}
# dev.off()
#dev.off()
par(mfrow=c(2,2)) # 4 pnales for plots
plot(u_sim[,1], type='o', pch=19, col="blue", main=TeX('AR innovation: $u_{x,t}$'), xlab='time', ylab='', cex = 0.5)
abline(0,0)
plot(u_sim[,2], type='o', pch=19, col="red", main=TeX('AR innovation: $u_{y,t}$'), xlab='time', ylab='', cex = 0.5)
abline(0,0)

# ACFs
lag.max.acf = 30
acf(u_sim[,1], main=TeX('ACF, AR: $x_t = \\phi_1 x_{t-1} + u_{x,t}$'), lag.max = lag.max.acf, xlab = "lag ") #, xlim = c(1,40)
acf(u_sim[,2], main=TeX('ACF, AR: $y_t = \\phi_2 y_{t-1} + u_{y,t}$'), lag.max = lag.max.acf, xlab = "lag ") # xlim = c(1,40)

# scatterplot + regression line
# dev.off()
par(mfrow=c(1,1)) # 4 panels for plots

#################################################
# scatterplot of u_yt vs. u_xt
#################################################
par(mfrow=c(1,1))
plot(x = u_sim[,1], y = u_sim[,2], 
     col = "blue", lwd =1, 
     xlab=TeX("$u_{x,t}$"), ylab=TeX("$u_{y,t}$"),
     main=TeX('$u_{y,t}$ vs. $u_{x,t}$'),   )

#dev.off()
reg1  <- lm(u_sim[,2] ~ u_sim[,1])
#res <-resid(reg1) ; 
# summary(reg1)

#stargazer(list(reg1,reg2,reg3f),type="text")
stargazer(list(reg1),type="text")
summary(reg1)
abline(reg1, col="red", lwd=2) # display regression line on scatterplot (y~x)
########################################################
# plot simulated AR(1)
# dev.off()
#dev.off()
par(mfrow=c(2,2)) # 4 whiten noises
plot(AR_sim[,1], type='o', pch=19, col="blue", main=TeX('AR: $x_t = \\phi_1 x_{t-1} + u_{x,t}$'), xlab='time', ylab='', cex = 0.5)
abline(0,0)
plot(AR_sim[,2], type='o', pch=19, col="red", main=TeX('AR: $y_t = \\phi_2 y_{t-1} + u_{y,t}$'), xlab='time', ylab='', cex = 0.5)
abline(0,0)


# legend("bottomleft", legend=c(TeX('AR: $x_t = 0.5 x_{t-1} + u_{x,t}$'), TeX('AR: $y_t = 0.95 y_{t-1} + u_{y,t}$')), 
#        pch=c(19,19), col=c("blue", "red"));
lag.max.acf = 30
acf(AR_sim[,1], main=TeX('ACF, AR: $x_t = \\phi_1 x_{t-1} + u_{x,t}$'), lag.max = lag.max.acf, xlab = "lag ") #, xlim = c(1,40)
acf(AR_sim[,2], main=TeX('ACF, AR: $y_t = \\phi_2 y_{t-1} + u_{y,t}$'), lag.max = lag.max.acf, xlab = "lag ") # xlim = c(1,40)
#################################################
# scatterplot of y_t vs. x_t
#################################################
par(mfrow=c(1,1))
plot(x = AR_sim[,1], y = AR_sim[,2], 
     col = "blue", lwd =1, 
     xlab=TeX("$x_{t}$"), ylab=TeX("$y_{t}$"),
     main=TeX('$y_{t}$ vs. $x_{t}$'),   )

#dev.off()
#############################################################
# Run Spurious regression ###################################
#############################################################
reg2  <- lm(AR_sim[,2] ~ AR_sim[,1])
#res <-resid(reg1) ; 
# summary(reg1)

#stargazer(list(reg1,reg2,reg3f),type="text")
stargazer(list(reg2),type="text")
summary(reg2)
abline(reg2, col="red", lwd=2) # display regression line on scatterplot (y~x)

stargazer(list(reg1,reg2), type = "text")
# Magics: you can save tabel directly in latex !!!!!!!!!!!!!!!!!!!
# stargazer(list(reg1,reg2),
#           align=TRUE, type = "text", no.space = TRUE, 
#           title = "Table X", out = "texoutput/fit.tex")
```



################################################################################
Define 2 random walk processes $x_t$ and $y_t$ analog to (14.03):
```{r RW_plot, echo=FALSE, message=FALSE, warning=FALSE, fig.show='hold', out.width='50%'}
RW <- function(Seed, N, x0, mu, sd) {
  # Seed = 201; N = 300; x0 = 0; mu = 0; sd = 1
  set.seed(Seed)
  z <- rnorm(n=N+1, mean=mu, sd=sd)
  x <- NULL; x[1] <- 0
  for (t in 2:(N+1)) x[t] <- x[t-1] + z[t]
  x <- x[-1]
  x <- as.data.frame(cbind(t = 1:N, x = x, u = z[-(N+1)]))
  return(x)
}
N=300
X.RW <- RW(Seed = 102, N = N, x0 = 0, mu = 0, sd = 1)
Y.RW <- RW(Seed = 201, N = N, x0 = 0, mu = 0, sd = 1)
rw_Data <- data.frame(cbind(t = X.RW$t, x = X.RW$x, u = X.RW$u, y = Y.RW$x, v = Y.RW$u))

lbl <- TeX('RW: $X_t = X_{t-1} + u_{t}$ / WN: $u_{t} \\sim iidN(0,1)$')
X.graph <- ggplot(rw_Data) +
  geom_line(aes(x = t, y = x), color = "blue") +
  geom_point(aes(x = t, y = x), color = "blue") +
  geom_line(aes(x = t, y = u), color = "red") +
  geom_point(aes(x = t, y = u), color = "red") +
  labs(x = 'time', y = 'x') +
  ggtitle('White Noise and Random Walk') +
  geom_hline(yintercept = 0) +
  annotate("label", x=100, y=-5, label = lbl) +
  theme_linedraw() +
  theme(text = element_text(size = 8))

lbl <- TeX('RW: $Y_t = Y_{t-1} + v_{t}$ / WN: $v_{t} \\sim iidN(0,1)$')
Y.graph <- ggplot(rw_Data) +
  geom_line(aes(x = t, y = y), color = "blue") +
  geom_point(aes(x = t, y = y), color = "blue") +
  geom_line(aes(x = t, y = v), color = "red") +
  geom_point(aes(x = t, y = v), color = "red") +
  labs(x = 'time', y = 'y') +
  ggtitle('White Noise and Random Walk') +
  geom_hline(yintercept = 0) +
  annotate("label", x=100, y=-5, label = lbl) +
  theme_linedraw() +
  theme(text = element_text(size = 8))

xy.Scatter <- ggplot(rw_Data, aes(x = x, y = y))+
  geom_point()

uv.Scatter <- ggplot(rw_Data, aes(x = u, y = v))+
  geom_point()

par(mfrow = c(4,1))
X.graph
Y.graph
xy.Scatter
uv.Scatter

```

We run a simple linear regression analog to (14.12) in the form of $y_t = \beta_1 + \beta_2 x_t + v_t$ and expect it to yield both $R^2$ as well as $\beta_2$ insignificantly different from 0. The hypotheses $H^1_0: \beta_2 = 0$ and $H^2_0: R^2 = 0$ will be tested by running 50 simple linear regressions on each sample size n ranging from 6 to 480, yielding a total of 100,000 regression outcomes. The 50 runs per sample will be based on individually seeded random walks using the command 'set.seed(1000+n)'. For each regression, the t-test $t = \frac{\beta_2 - 0}{\sigma_{\beta_2}} > 1.96$ indicates a rejection of the hypothesis. The rejection rate (no. of t-test > 1.96 / 50) will then be plotted against n in replication of figure 14.1.

```{r}
T <- c(6, 12, 60, 120, 240, 360, 480)
result <- c(t=NULL, R2=NULL, t_val=NULL, t_sig=NULL)
for (t in T){
  rejected_r2 <- NULL
  rejected_t <- NULL
  for (s in 1:150) {
    # n = 20; x0 = 0; mu = 0; sd = 1
    Seed <- t+s+100
    xt <- RW(Seed = Seed, N = t, x0 = 0, mu = 0, sd = 1)
    yt <- RW(Seed = 1/Seed, N = t, x0 = 0, mu = 0, sd = 1)
    su <- summary(lm(yt$x ~ xt$x))
    result <- rbind(result,c(t, su$r.squared, su$coefficients[2,3], ifelse(su$coefficients[2,3]>1.96,1,0)))
  }
}
colnames(result) <- c("t", "R2", "t_value", "t_sig")

result <- as_tibble(result) %>% 
  mutate(t = as.factor(t))

graph <- ggplot(result, aes(x = t, y = R2, colour = t)) +
  geom_boxplot(notch = FALSE) +
  geom_jitter(size = 1, alpha = 0.5, width = 0.25, colour = 'black') +
  theme_bw()
graph
```

