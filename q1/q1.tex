\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{blindtext}
\usepackage{multicol}
%opening
\title{}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section*{Exercise 1: Spurious Regression}
\subsection*{1. Set-up and Definitions}
\begin{itemize} 
	\item time series are generated as standardized random walk processes:
		\begin{align*}
			x_t &= \phi x_{t-1} + u_{x,t}, x_0=0, u_{x,t}  \overset{iid}{\sim}\mathcal{N}(0,1)\\
			y_t &= \phi y_{t-1} + u_{y,t}, y_0=0, u_{y,t}  \overset{iid}{\sim}\mathcal{N}(0,1)\\
			\phi &\in \left\{0.8, 1\right\}
		\end{align*}
		A DGP with $\phi=0.8$ is referred to as \textbf{AR(1)} and a DGP with $\phi=1$ as \textbf{random walk} in the text.
	\item two regression models are defined:
		\begin{align*}
			y_t &= \beta_1 +\beta_2 x_t + v_t, &(\text{spurious regression})\\
			y_t &= \beta_1 + \beta_2 x_t + \beta_3 y_{t-1} + v_t, &(\text{valid regression})
		\end{align*}
		The \textbf{spurious regression} model should be expected to produce an insignificant estimate of $\beta_2$ and an $R^2$ near 0.
		The \textbf{valid regression} model reduces to the DGP for $y_t$ given $\beta_1=\beta_2=0$ and $\beta_3=1$.
	\item The combination of above DGPs and regression models yield 4 permutations for which the authors report how the rejection frequencies of $H0: \beta_2=0$ change with increasing sample size n.
	\begin{align*}
		LRM_1: y_{t} &= \beta_{1} +\beta_{2}x_{t} + v_{t}, &x_t, y_t: I(1), \phi=1\\
		LRM_2: y_{t} &= \beta_{1} +\beta_{2}x_{t} + v_{t}, &x_t, y_t: AR(1), \phi=0.8\\
		LRM_3: y_{t} &= \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}, &x_t, y_t: I(1), \phi=1\\
		LRM_4: y_{t} &= \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}, &x_t, y_t: AR(1), \phi=0.8
	\end{align*}	
	\item In each simulation the DGP produce timeseries of length T = 6, 12, 60, 120, 240, 360, 480.
	\item A total of $N_{sim} = 100,000$ is run to produce the distributions of t-stat and $R^2$.
	\item Each regression model is estimated with OLS under asymptotic theory\footnote{ref: class notes, Ch 2, p. 60}
	\subitem \textbf{A.1: independent observations} $y_i, x_i$ are i.i.d, which is equivalent to $v_t|x$ and $v_t|y$ being i.i.d.
	\subitem \textbf{A.2: regressors are uncorrelated with errors} $E[v_t x_t]=E[v_t y_t]=0$. This assumption is weaker than strict exogeneity and is not restricting the utilization of lagged values of the dependent variable into the regressors matrix.
	\subitem \textbf{A.3 Identification} $Q= E[x_{t}x_{t}']$ exists, is positive definite and has rank corresponding to the total number of regressors (no regressor is a linear combination of the others).
	\subitem \textbf{A.4: Finite Moments } $S = E[v_{t}^2x_{t}x_{t}']$ exists and is positive definite, with 
		\begin{align*}
			S &= E[v_{t}^2x_{t}x_{t}']\\
			&= E[v_t^2]E[x_tx_t']+cov(v_t,x_t)\\
			&= E[v_t^2]E[Q], by A.2
		\end{align*}
\end{itemize}

\newpage
\subsection*{1.a Replication of Figure 14.1 in Davidson MacKinnon (2005, book) }
\subsubsection*{(i) Compute for each sample size T the distribution of the $R^2$ of the MC simulations with either 7 separate histograms, or one unique figure where you report on the y-axis the 5\%, 10\% 25\%, 50\%, 75\%, 90\% and 95\% quantiles of the distributions of the simulated $R^2$, and on the x-axis you have T = 6, 12, 60, 120, 240, 360, 480.}

$LRM_1$ spurious regression of random walks with $\phi=1$ should produce $R^2$ near 0 given that $\beta_2$ is expected to be insignificant. The distribution of simulated $R^2$ however indicates that this is not the case for any length of timeseries. 

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1ai_chart"}
	\caption[]{Distribution of $R^2$}
\end{figure}

\newpage
\subsubsection*{(ii) Similarly (either with histograms, or with one plot of the quantiles) report the distributions of the estimates t-statistics for the test of the null H0 : $\beta_2 = 0$}

The dispersion of the t-statistic $t_{\beta_2} = \frac{\beta_2 - 0}{\sigma_{\beta_2}}$ around 0 increases with increasing length of the timeseries.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1aii_chart"}
	\caption[]{Distribution of $t_{\beta_2}$}
\end{figure}

\newpage
\subsubsection*{(iii) Compute the empirical rejection frequencies (that is the empirical size of the tests), which is exactly the figure 14.1 in Davidson MacKinnon (2005, book).}

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1aiii_chart"}
	\caption[]{Rejection rate of $H0: \beta_2=0$}
\end{figure}

\subsection*{1b: Summarize the problems of spurious regressions in econometrics}

\begin{multicols}{2}
	bla bli blub
	
	\blindtext 
	\columnbreak

\end{multicols}

The results obtained are different from what we previously expected. Indeed, by looking at Figure 1.3, we can observe that:
\begin{itemize}
	\item Apart from DPG 4 whose $H_0$ rejection rate converges asymptotically to 5\%, the empirical rejection rate is structurally higher than the significance level for the three other processes.
	\item Additionally, we notice that the proportion of rejections keeps increasing alongside the sample size for DPG 1 always implying a statistically significant relationship bwetween $y_t$ and $x_t$ even if this does not exist.
	\item Therefore, it emerges that the actual probability of a Type 1 error\footnote{$P(R_{H_0}|H_0)$.} is significantly higher than the assumed test size.
\end{itemize}
Usually, the bias of the $\beta$ multipled by $n^{0.5}$ is $O_{p}(1)$, as per below demonstration and noting that $K'K = O_{p}(T)$ and $K'v = O_{p}(T^{0.5})$:
\begin{align*}
	\hat{\beta} - \beta &= (K'K)^{-1}K'v \\
	T^{0.5}(\hat{\beta} - \beta) &= T^{0.5}((K'K)^{-1}K'v) \\
	&= T^{0.5}O_{p}(T^{-1}O_{p}(T^{0.5}) \\
	&= O_{p}(1)  
\end{align*}
The bias is eventually bounded for $T \to +\infty$, so it does not explode asymptotically.\\ 
However, we have here an issue of unit roots and spurious regressions. If, at least, one of the regressors is a unit root\footnote{This happens for DPG 1 and 3, although for the latter the proportion of rejections does not coverge to 1 as $t$ increases. This is because we are reducing $y_t$ by one lag turning the dependent variable into a white noise random walk.}, we can derive that $E[K_{t}'K_{t}]$ is not a finite positive definite matrix anymore and there is a violation of the OLS assumptions under asymptotic theory (steps below are for DPG 1\footnote{$V(x_t) = E(x_{t}^2) = t$.}, same holds in case of multiple regressors):
\begin{align*}
	E[x_{T}'x_{T}] &= \sum_{t=1}^{T}x_{t}^2 \\  
	&= T + (T-1) +...+2+1 \\
	&= \frac{T(T+1)}{2}
\end{align*}
It follows that: i) $T^{-1}x_{T}'x_{T}$ is $O(T)$ and ii) this metric does not have a finite probability limit ($T \to + \infty$). \\
The distribution of the t-statistics does not converge to the Student's $t$ even asymptotically causing an overrejection of the null.\\
Eventually, we observe that the issue of spurious regressions occurs even if all variables are stationary\footnote{AR(1) in this case.}.
Although the proportion of rejections converges to a fixed number and $E[K_{t}'K_{t}]$ is a finite definite positive metric, we can see that the result for DPG 2 implies an overrejection of the null while the empirical frequency for DPG 4, the only correct regression model, converges to the $5\%$ significance level only after increasing significantly the sample size (in the order of a few thousand observations). \\  
The distortion for DPG 3 arises from the fact that neither the constant nor $x_t$ have any explanatory power for $y_t$, therefore $y_t = v_t = 0.8y_{t-1}+u_{y,t}$.\\ As $y_t$ is modelled as an AR(1), the error term of DPG 3 becomes:
\begin{align*}
	v_t &= 0.8y_{t-1}+u_{y,t} \\
	v_t &= 0.8(u_{y,t-1} + 0.8y_{t-2}) + +u_{y,t} \; t=1,...,T
\end{align*}
This intuition suggests that there is serial autocorrelation in the errors and the (asymptotic) solution to avoid too small standard errors in finite samples is to switch from the OLS VaR-Cov estimator to the Newey-West heteroskedasticity and autocorrelation consistent (HAC) one.

\end{document}
