\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}


%\usepackage{easyReview} %used for annotation. Cannot be run together with (changes) - command "highlight" defined twice
\usepackage{changes}%use while actively annotating

%\usepackage[final]{changes} % once we are happy with all annotation, we can accept all proposed changes and show only the final document.

% simply select text you want to annotate and start typing the command \replacedGM{the selected text will appear here}{and here you enter the new replacement text}
\definechangesauthor[name={Timo}, color=orange]{tp}
\definechangesauthor[name={Giovanni}, color=green]{gm}
\newcommand{\replacedTP}[2]{\replaced[id=tp]{#2}{#1}}
\newcommand{\addedTP}[1]{\added[id=tp]{#1}}
\newcommand{\deletedTP}[1]{\deleted[id=tp]{#1}}
\newcommand{\highlightTP}[1]{\highlight[id=tp]{#1}}
\newcommand{\commentTP}[2]{\comment[id=tp]{#2}{#1}}

\newcommand{\replacedGM}[2]{\replaced[id=gm]{#2}{#1}}
\newcommand{\addedGM}[1]{\added[id=gm]{#1}}
\newcommand{\deletedGM}[1]{\deleted[id=gm]{#1}}
\newcommand{\highlightGM}[1]{\highlight[id=gm]{#1}}
\newcommand{\commentGM}[2]{\comment[id=gm]{#2}{#1}}

%opening
\title{}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section*{Exercise 1: Spurious Regression}
\subsection*{1. Set-up and Definitions}
\begin{itemize} 
	\item time series are generated as standardized random walk processes:
		\begin{align*}
			x_t &= \phi x_{t-1} + u_{x,t}, x_0=0, u_{x,t}  \overset{iid}{\sim}\mathcal{N}(0,1)\\
			y_t &= \phi y_{t-1} + u_{y,t}, y_0=0, u_{y,t}  \overset{iid}{\sim}\mathcal{N}(0,1)\\
			\phi &\in \left\{0.8, 1\right\}
		\end{align*}
		A DGP with $\phi=0.8$ is referred to as \textbf{AR(1)} and a DGP with $\phi=1$ as \textbf{random walk} in the text.
	\item two regression models are defined:
		\begin{align*}
			y_t &= \beta_1 +\beta_2 x_t + v_t, &(\text{spurious regression})\\
			y_t &= \beta_1 + \beta_2 x_t + \beta_3 y_{t-1} + v_t, &(\text{valid regression})
		\end{align*}
		The \textbf{spurious regression} model should be expected to produce an insignificant estimate of $\beta_2$ and an $R^2$ near 0.
		The \textbf{valid regression} model reduces to the DGP for $y_t$ given $\beta_1=\beta_2=0$ and $\beta_3=1$.
	\item The combination of above DGPs and regression models yield 4 permutations for which the authors report how the rejection frequencies of $H0: \beta_2=0$ change with increasing sample size n.
	\begin{align*}
		LRM_1: y_{t} &= \beta_{1} +\beta_{2}x_{t} + v_{t}, &x_t, y_t: I(1), \phi=1\\
		LRM_2: y_{t} &= \beta_{1} +\beta_{2}x_{t} + v_{t}, &x_t, y_t: AR(1), \phi=0.8\\
		LRM_3: y_{t} &= \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}, &x_t, y_t: I(1), \phi=1\\
		LRM_4: y_{t} &= \beta_{1} +\beta_{2}x_{t} + \beta_{3}y_{t-1} + v_{t}, &x_t, y_t: AR(1), \phi=0.8
	\end{align*}	
	\item In each simulation the DGP produce timeseries of length T = 6, 12, 60, 120, 240, 360, 480.
	\item A total of $N_{sim} = 100,000$ is run to produce the distributions of t-stat and $R^2$.
	\item Each regression model is estimated with OLS under asymptotic theory\footnote{ref: class notes, Ch 2, p. 60}
	\subitem \textbf{A.1: independent observations} $y_i, x_i$ are i.i.d, which is equivalent to $v_t|x$ and $v_t|y$ being i.i.d.
	\subitem \textbf{A.2: regressors are uncorrelated with errors} $E[v_t x_t]=E[v_t y_t]=0$. This assumption is weaker than strict exogeneity and is not restricting the utilization of lagged values of the dependent variable into the regressors matrix.
	\subitem \textbf{A.3 Identification} $Q= E[x_{t}x_{t}']$ exists, is positive definite and has rank corresponding to the total number of regressors (no regressor is a linear combination of the others).
	\subitem \textbf{A.4: Finite Moments } $S = E[v_{t}^2x_{t}x_{t}']$ exists and is positive definite, with 
		\begin{align*}
			S &= E[v_{t}^2x_{t}x_{t}']\\
			&= E[v_t^2]E[x_tx_t']+cov(v_t,x_t)\\
			&= E[v_t^2]E[Q], by A.2
		\end{align*}
\end{itemize}

\newpage
\subsection*{1.a Replication of Figure 14.1 in Davidson MacKinnon (2005, book) }
\subsubsection*{(i) Compute for each sample size T the distribution of the $R^2$ of the MC simulations with either 7 separate histograms, or one unique figure where you report on the y-axis the 5\%, 10\% 25\%, 50\%, 75\%, 90\% and 95\% quantiles of the distributions of the simulated $R^2$, and on the x-axis you have T = 6, 12, 60, 120, 240, 360, 480.}

$LRM_1$ spurious regression of random walks with $\phi=1$ should produce $R^2$ near 0 given that $\beta_2$ is expected to be insignificant. The distribution of simulated $R^2$ however indicates that this is not the case for any length of timeseries. 

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1ai_chart"}
	\caption[]{Distribution of $R^2$}
\end{figure}

\newpage
\subsubsection*{(ii) Similarly (either with histograms, or with one plot of the quantiles) report the distributions of the estimates t-statistics for the test of the null H0 : $\beta_2 = 0$}

The dispersion of the t-statistic $t_{\beta_2} = \frac{\beta_2 - 0}{\sigma_{\beta_2}}$ around 0 increases with increasing length of the timeseries.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1aii_chart"}
	\caption[]{Distribution of $t_{\beta_2}$}
\end{figure}

\newpage
\subsubsection*{(iii) Compute the empirical rejection frequencies (that is the empirical size of the tests), which is exactly the figure 14.1 in Davidson MacKinnon (2005, book).}

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm, height=12cm]{"./1aiii_chart"}
	\caption[]{Rejection rate of $H0: \beta_2=0$}
\end{figure}

\newpage
\subsection*{1b: Summarize the problems of spurious regressions in econometrics}

The results obtained are different from what we previously expected. Indeed, by looking at Figure 3, we can observe that:
\begin{itemize}
	\item The simulated rate of rejection in $LRM_4$ converges asymptotically to 5\% and significantly higher for the three other $LRM_{1-3}$.
	\item Additionally, we notice that the proportion of rejections keeps increasing in T for $LRM_1$ and $LRM_2$ implying a statistically significant relationship ($\beta_2$) between $y_t$ and $x_t$ even if this should not exist.
	\item Therefore, it emerges that the actual probability of a mistaken rejection of the true H0, i.e. committing a Type 1 error\footnote{$P(R_{H_0}|H_0)$.} is significantly higher than the assumed \commentTP{test size}{T?}.
\end{itemize}

Issues with these regressions:
\begin{itemize}
	\item The bias of $\beta_2$ does not converge asymptotically in probability. 
	\begin{itemize}
		\item For $\hat{\beta}$ to converge to $\beta_0$ asymptotically, the bias $(\hat{\beta} - \beta_0)$ must be $O_p(1)$: 
		\begin{align*}
			(\hat{\beta} - \beta_0) &= (X'X)^{-1}X'u, &\text{with}\\
			(X'X)^{-1} &\in O_p(n^{-1}) &\text{and} \\
			X'u &\in O_p(n^{.5}) &\text{consequently:}\\
			n^{.5}(\hat{\beta} - \beta_0) &= n^{.5}(X'X)^{-1}X'u = n^{.5} O_p(n^{-1})O_p(n^{.5}) = O_p(1)
		\end{align*}
		\item The relevant assumption to be tested is therefore is $(X'X) \in O_p(n)$.
		\item The random walks found in $LRM_1$ and $LRM_3$ are I(1), due to: \begin{align*}
			w_t &= w_{t-1}+\epsilon_t \\
			w_t - w_{t-1} &= \epsilon_t \\
			(1-L)w_t  &= \epsilon_t \\
			(1-\phi(z))w_t &= \epsilon_t \\
			\phi(z) &= 1
		\end{align*} Consequently, both $x_t$ and $y_t$ are I(1) given $\phi=1$.
		\item Further, $LRM_1$ and $LRM_3$ reduce recursively to $w_t = \sum^t_{s=1}{\epsilon_s}$, which enters as $X'X$ or: \begin{align*}
			\sum_{t=1}^n \left( \sum_{r=1}^t \sum_{s=1}^t \right) \epsilon_r \epsilon_s &= \sum_{t=1}^n \sum_{r=1}^t E(\epsilon_r^2), \epsilon_r\epsilon_s=0 \;\forall r \ne s\\
			\sum_{t=1}^n \sum_{r=1}^t \sigma^2 &= \sum_{t=1}^n \sum_{r=1}^t 1 , \text{by assumption}\\
			\sum_{t=1}^n t  &= \frac{1}{2}n(n+1)
		\end{align*}
		\item Consequently, being I(1), $X'X \in O_p(n^2)$ and therefore cannot possibly converge to a finite probability limit. The bias $(\hat{\beta} - \beta_0)$ therefore does not converge asymptotically in probability.
	\end{itemize}
	
	\item 
\end{itemize}

\highlightTP{Text from Giovanni}\\
Usually, the bias of the $\beta$ multipled by $n^{0.5}$ is $O_{p}(1)$, as per below demonstration and noting that $K'K = O_{p}(T)$ and $K'v = O_{p}(T^{0.5})$:
\begin{align*}
	\hat{\beta} - \beta &= (K'K)^{-1}K'v \\
	T^{0.5}(\hat{\beta} - \beta) &= T^{0.5}((K'K)^{-1}K'v) \\
	&= T^{0.5}O_{p}(T^{-1}O_{p}(T^{0.5}) \\
	&= O_{p}(1)  
\end{align*}
The bias is eventually bounded for $T \to +\infty$, so it does not explode asymptotically.\\ 
However, we have here an issue of unit roots and spurious regressions. If, at least, one of the regressors is a \commentTP{unit root}{Both $LRM_3$ and $LRM_1$ have a unit root. But $LRM_1$ is also wrongly specified, ie does not reduce to the DGP}\footnote{This happens for $LRM_1$ and $LRM_3$, although for the latter the proportion of rejections does not converge to 1 as T increases. This is because we are reducing $y_t$ by one lag turning the dependent variable into a white noise random walk}, we can derive that $E[K_{t}'K_{t}]$ is not a finite positive definite matrix anymore and there is a violation of the OLS assumptions under asymptotic theory (steps below are for DPG 1\footnote{$V(x_t) = E(x_{t}^2) = t$.}, same holds in case of multiple regressors):
\begin{align*}
	E[x_{T}'x_{T}] &= \sum_{t=1}^{T}x_{t}^2 \\  
	&= T + (T-1) +...+2+1 \\
	&= \frac{T(T+1)}{2}
\end{align*}
It follows that: i) $T^{-1}x_{T}'x_{T}$ is $O(T)$ and ii) this metric does not have a finite probability limit ($T \to + \infty$). \\
The distribution of the t-statistics does not converge to the Student's $t$ even asymptotically causing an overrejection of the null.\\
Eventually, we observe that the issue of spurious regressions occurs even if all variables are stationary\footnote{AR(1) in this case.}.
Although the proportion of rejections converges to a fixed number and $E[K_{t}'K_{t}]$ is a finite definite positive metric, we can see that the result for DPG 2 implies an overrejection of the null while the empirical frequency for DPG 4, the only correct regression model, converges to the $5\%$ significance level only after increasing significantly the sample size (in the order of a few thousand observations). \\  
The distortion for DPG 3 arises from the fact that neither the constant nor $x_t$ have any explanatory power for $y_t$, therefore $y_t = v_t = 0.8y_{t-1}+u_{y,t}$.\\ As $y_t$ is modelled as an AR(1), the error term of DPG 3 becomes:
\begin{align*}
	v_t &= 0.8y_{t-1}+u_{y,t} \\
	v_t &= 0.8(u_{y,t-1} + 0.8y_{t-2}) + +u_{y,t} \; t=1,...,T
\end{align*}
This intuition suggests that there is serial autocorrelation in the errors and the (asymptotic) solution to avoid too small standard errors in finite samples is to switch from the OLS VaR-Cov estimator to the Newey-West heteroskedasticity and autocorrelation consistent (HAC) one.


\highlightTP{Text from Timo}\\
The $H_0: \beta_2=0$ tested with the model (14.12) $$y_t = \beta_1 + \beta_2y_{t-1} + v_t$$ implies a DGP': $y_t = \beta_1 + v_t$, when $y_t$ is actually generated using DGP (14.01) $y_t = y_{t-1} + v_t, y_0=0, v_t \sim iidN(0,1)$.

The wrongly specified $H_0$ is rejected with increasing frequency in n. This merely confirms that $y_t$ is not generated by the implied DGP'. Correctly specifying the model as (14.13) $$y_t = \beta_1 + \beta_2 x_t + \beta_3 y_{t-1} + v_t$$ and testing $H_0:\beta_2=0$, implying $\beta_3=1$ reduces the model to the actual DGP (14.01). This treatment, however, does not completely eliminate the problem i.e. leaves the rejection rate still significantly above 0.





\end{document}
